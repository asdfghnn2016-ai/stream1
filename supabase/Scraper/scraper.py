"""
scraper.py â€” Ø³ÙƒØ±Ø§Ø¨Ø± Ù†ØªØ§Ø¦Ø¬ Ø§Ù„Ù…Ø¨Ø§Ø±ÙŠØ§Øª Ø§Ù„Ù…Ø¨Ø§Ø´Ø±Ø© Ù…Ù† Yallakora
ÙŠØ´ØªØºÙ„ ÙÙŠ Ø­Ù„Ù‚Ø© Ù…Ø³ØªÙ…Ø±Ø© Ù„Ù…Ø¯Ø© 5 Ø¯Ù‚Ø§Ø¦Ù‚ (280 Ø«Ø§Ù†ÙŠØ©)
ÙƒÙ„ 30 Ø«Ø§Ù†ÙŠØ© ÙŠØ¬Ù„Ø¨ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª ÙˆÙŠØ­Ø¯Ù‘Ø« Ù‚Ø§Ø¹Ø¯Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª

Ø§Ù„Ø§Ø³ØªØ®Ø¯Ø§Ù…:
  python scraper.py              # ØªØ´ØºÙŠÙ„ Ø§Ù„Ø­Ù„Ù‚Ø© Ø§Ù„ÙƒØ§Ù…Ù„Ø©
  python scraper.py --once       # ØªØ´ØºÙŠÙ„ Ù…Ø±Ø© ÙˆØ§Ø­Ø¯Ø© ÙÙ‚Ø· (Ù„Ù„Ø§Ø®ØªØ¨Ø§Ø±)
  python scraper.py --dry-run    # Ø¬Ù„Ø¨ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø¨Ø¯ÙˆÙ† ØªØ­Ø¯ÙŠØ« Ø§Ù„Ù€ DB
"""

import sys
import time
import logging
import argparse
import re
from datetime import datetime, timezone, timedelta

try:
    from dotenv import load_dotenv
    load_dotenv()
except ImportError:
    pass  # ÙÙŠ Ø¨ÙŠØ¦Ø© CI/CD Ø§Ù„Ù€ env vars ØªÙƒÙˆÙ† Ø¬Ø§Ù‡Ø²Ø©

from playwright.sync_api import sync_playwright

from database import get_session, upsert_matches

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# Configuration
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

YALLAKORA_URL = "https://www.yallakora.com/match-center/"
LOOP_DURATION_SECONDS = 280   # 4 Ø¯Ù‚Ø§Ø¦Ù‚ Ùˆ 40 Ø«Ø§Ù†ÙŠØ© (Ø£Ù‚Ù„ Ù…Ù† GitHub Actions timeout)
SLEEP_INTERVAL_SECONDS = 30   # ÙƒÙ„ 30 Ø«Ø§Ù†ÙŠØ©
USER_AGENT = (
    "Mozilla/5.0 (Windows NT 10.0; Win64; x64) "
    "AppleWebKit/537.36 (KHTML, like Gecko) "
    "Chrome/131.0.0.0 Safari/537.36"
)

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# Logging
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

logging.basicConfig(
    level=logging.INFO,
    format="%(asctime)s | %(levelname)-7s | %(message)s",
    datefmt="%H:%M:%S",
)
logger = logging.getLogger(__name__)


# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# Scraping Logic
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€


def parse_score(score_text: str) -> tuple[int, int]:
    """ØªØ­ÙˆÙŠÙ„ Ù†Øµ Ø§Ù„Ù†ØªÙŠØ¬Ø© Ø¥Ù„Ù‰ Ø£Ø±Ù‚Ø§Ù…. Ù…Ø«Ø§Ù„: '2 - 1' â†’ (2, 1)"""
    score_text = score_text.strip()
    # Try common separators
    for sep in ["-", "â€“", ":"]:
        if sep in score_text:
            parts = score_text.split(sep)
            try:
                return int(parts[0].strip()), int(parts[1].strip())
            except (ValueError, IndexError):
                continue
    return 0, 0


def parse_minute(minute_text: str) -> int:
    """
    ØªØ­ÙˆÙŠÙ„ Ù†Øµ Ø§Ù„Ø¯Ù‚ÙŠÙ‚Ø© Ø¥Ù„Ù‰ Ø±Ù‚Ù….
    Ø£Ù…Ø«Ù„Ø©: "45'" â†’ 45, "45+2'" â†’ 47, "HT" â†’ 45, "FT" â†’ 90
    """
    minute_text = minute_text.strip().replace("'", "").replace("â€²", "")

    if minute_text in ("HT", "Ø´.Ø£", "Ù†Ù‡Ø§ÙŠØ© Ø§Ù„Ø´ÙˆØ·"):
        return 45
    if minute_text in ("FT", "Ù†.Ù…", "Ù†Ù‡Ø§ÙŠØ© Ø§Ù„Ù…Ø¨Ø§Ø±Ø§Ø©"):
        return 90

    # Handle "45+2" format
    match = re.match(r"(\d+)\+(\d+)", minute_text)
    if match:
        return int(match.group(1)) + int(match.group(2))

    try:
        return int(minute_text)
    except ValueError:
        return 0


def map_status(status_text: str) -> str:
    """ØªØ­ÙˆÙŠÙ„ Ø­Ø§Ù„Ø© Ø§Ù„Ù…Ø¨Ø§Ø±Ø§Ø© Ù…Ù† Ø§Ù„Ù†Øµ Ø§Ù„Ø¹Ø±Ø¨ÙŠ/Ø§Ù„Ø¥Ù†Ø¬Ù„ÙŠØ²ÙŠ Ø¥Ù„Ù‰ enum Ø§Ù„Ù€ DB."""
    status_lower = status_text.strip().lower()

    live_keywords = [
        "live", "Ù…Ø¨Ø§Ø´Ø±", "Ø¬Ø§Ø±ÙŠØ©", "Ø§Ù„Ø´ÙˆØ·", "Ø´ÙˆØ·", "Ø´.Ø£", "Ø´.Ø«",
        "Ø¨Ø¯Ø£Øª", "Ø§Ø³ØªØ±Ø§Ø­Ø©", "ht", "extra", "Ø¥Ø¶Ø§ÙÙŠ",
    ]
    finished_keywords = [
        "finished", "ft", "Ø§Ù†ØªÙ‡Øª", "Ù†.Ù…", "Ù†Ù‡Ø§ÙŠØ©", "ended",
    ]

    for kw in live_keywords:
        if kw in status_lower:
            return "live"
    for kw in finished_keywords:
        if kw in status_lower:
            return "finished"

    return "upcoming"


def scrape_matches(page) -> list[dict]:
    """
    Ø¬Ù„Ø¨ Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ù…Ø¨Ø§Ø±ÙŠØ§Øª Ù…Ù† ØµÙØ­Ø© Yallakora.
    ÙŠØ±Ø¬Ø¹ Ù‚Ø§Ø¦Ù…Ø© dictionaries Ø¬Ø§Ù‡Ø²Ø© Ù„Ù„Ù€ upsert.
    """
    matches = []
    today = datetime.now(timezone.utc).date()

    try:
        # Navigate
        page.goto(YALLAKORA_URL, wait_until="domcontentloaded", timeout=30000)
        page.wait_for_timeout(3000)  # Wait for JS to render

        # Try multiple selectors (Yallakora changes layout occasionally)
        match_containers = page.query_selector_all(
            ".matchCard, .match-card, .liItem, .item, [class*='match']"
        )

        if not match_containers:
            # Fallback: try the main content area
            match_containers = page.query_selector_all(
                "#matchesContainer .item, .allData .item, .matchesList .item"
            )

        logger.info(f"ğŸ“¦ ÙˆÙØ¬Ø¯ {len(match_containers)} Ø¹Ù†ØµØ± Ù…Ø¨Ø§Ø±Ø§Ø© ÙÙŠ Ø§Ù„ØµÙØ­Ø©")

        for container in match_containers:
            try:
                match_data = extract_match_from_element(container, today)
                if match_data:
                    matches.append(match_data)
            except Exception as e:
                logger.debug(f"â­ï¸ ØªØ®Ø·ÙŠ Ø¹Ù†ØµØ±: {e}")
                continue

    except Exception as e:
        logger.error(f"âŒ Ø®Ø·Ø£ ÙÙŠ Ø¬Ù„Ø¨ Ø§Ù„ØµÙØ­Ø©: {e}")

    return matches


def extract_match_from_element(element, today) -> dict | None:
    """
    Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø¨ÙŠØ§Ù†Ø§Øª Ù…Ø¨Ø§Ø±Ø§Ø© ÙˆØ§Ø­Ø¯Ø© Ù…Ù† Ø¹Ù†ØµØ± HTML.
    ÙŠØ­Ø§ÙˆÙ„ Ø¹Ø¯Ø© selectors Ù…Ø®ØªÙ„ÙØ©.
    """
    # â”€â”€ Team Names & Logos â”€â”€
    team_elements = element.query_selector_all(
        ".teamName, .team-name, .teamA, .teamB, .team, [class*='team']"
    )
    
    home_team = ""
    away_team = ""
    home_logo = ""
    away_logo = ""

    if len(team_elements) >= 2:
        # Extract name
        home_team = team_elements[0].query_selector(".name, span, strong") or team_elements[0]
        home_team = home_team.inner_text().strip()
        
        away_team = team_elements[1].query_selector(".name, span, strong") or team_elements[1]
        away_team = away_team.inner_text().strip()

        # Extract logo
        home_img = team_elements[0].query_selector("img")
        if home_img:
            home_logo = home_img.get_attribute("src") or home_img.get_attribute("data-src") or ""
            
        away_img = team_elements[1].query_selector("img")
        if away_img:
            away_logo = away_img.get_attribute("src") or away_img.get_attribute("data-src") or ""

    if not home_team or not away_team:
        # Fallback to text splitting if selectors fail
        all_text = element.inner_text().strip().split("\n")
        all_text = [t.strip() for t in all_text if t.strip()]
        # Heuristic: Find localized text
        if len(all_text) >= 2:
             # Basic fallback, no logos here
             home_team = all_text[0] 
             away_team = all_text[-1]

    if not home_team or not away_team:
        return None

    # â”€â”€ Score â”€â”€
    score_el = element.query_selector(
        ".score, .result, .matchResult, [class*='score'], [class*='result']"
    )
    home_score, away_score = 0, 0
    if score_el:
        score_text = score_el.inner_text().strip()
        home_score, away_score = parse_score(score_text)

    # â”€â”€ Status / Minute â”€â”€
    status_el = element.query_selector(
        ".matchStatus, .status, .time, .matchTime, [class*='status'], [class*='live']"
    )
    status_text = ""
    minute = 0
    if status_el:
        status_text = status_el.inner_text().strip()
        status = map_status(status_text)
        if status == "live":
            minute = parse_minute(status_text)
    else:
        status = "upcoming"

    # â”€â”€ League â”€â”€
    league_el = element.query_selector(
        ".championship, .league, .tournamentName, .tourName, [class*='champ'], [class*='league']"
    )
    league_name = league_el.inner_text().strip() if league_el else ""

    # â”€â”€ Time â”€â”€
    time_el = element.query_selector(
        ".matchTime, .time, [class*='time']"
    )
    match_time = datetime.now(timezone.utc)
    if time_el and status == "upcoming":
        time_text = time_el.inner_text().strip()
        time_match = re.search(r"(\d{1,2}):(\d{2})", time_text)
        if time_match:
            hour, mins = int(time_match.group(1)), int(time_match.group(2))
            match_time = datetime.combine(
                today,
                datetime.min.time().replace(hour=hour, minute=mins),
                tzinfo=timezone.utc,
            )

    # â”€â”€ Channel â”€â”€
    channel_el = element.query_selector(
        ".channel, [class*='channel'], [class*='broadcaster']"
    )
    channel = channel_el.inner_text().strip() if channel_el else ""

    # â”€â”€ Round â”€â”€
    round_el = element.query_selector(
        ".round, .matchRound, [class*='round'], [class*='week']"
    )
    round_name = round_el.inner_text().strip() if round_el else ""

    return {
        "home_team_name": home_team,
        "away_team_name": away_team,
        "home_team_logo": home_logo,
        "away_team_logo": away_logo,
        "home_score": home_score,
        "away_score": away_score,
        "status": status,
        "minute": minute,
        "league_name": league_name,
        "start_time": match_time,
        "channel": channel,
        "round": round_name,
    }


# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# Main Loop
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€


def run_scraper(once: bool = False, dry_run: bool = False):
    """
    Ø­Ù„Ù‚Ø© Ø§Ù„Ø³ÙƒØ±Ø§Ø¨Ø± Ø§Ù„Ø±Ø¦ÙŠØ³ÙŠØ©.
    
    Args:
        once: ØªØ´ØºÙŠÙ„ Ù…Ø±Ø© ÙˆØ§Ø­Ø¯Ø© ÙÙ‚Ø· (Ù„Ù„Ø§Ø®ØªØ¨Ø§Ø±)
        dry_run: Ø¬Ù„Ø¨ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø¨Ø¯ÙˆÙ† ØªØ­Ø¯ÙŠØ« Ø§Ù„Ù€ DB
    """
    logger.info("=" * 50)
    logger.info("âš½ Ø³ÙƒØ±Ø§Ø¨Ø± Kora â€” Ø¨Ø¯Ø§ÙŠØ© Ø§Ù„ØªØ´ØºÙŠÙ„")
    logger.info(f"   Ø§Ù„ÙˆØ¶Ø¹: {'Ù…Ø±Ø© ÙˆØ§Ø­Ø¯Ø©' if once else f'Ø­Ù„Ù‚Ø© {LOOP_DURATION_SECONDS}s'}")
    logger.info(f"   Ø§Ù„Ù€ DB: {'Ù…Ø¹Ø·Ù‘Ù„ (dry-run)' if dry_run else 'Ù…ÙØ¹Ù‘Ù„'}")
    logger.info("=" * 50)

    start_time = time.time()
    iteration = 0

    with sync_playwright() as p:
        logger.info("ğŸŒ ØªØ´ØºÙŠÙ„ Ø§Ù„Ù…ØªØµÙØ­...")
        browser = p.chromium.launch(
            headless=True,
            args=[
                "--no-sandbox",
                "--disable-setuid-sandbox",
                "--disable-dev-shm-usage",
                "--disable-gpu",
            ],
        )
        context = browser.new_context(
            user_agent=USER_AGENT,
            viewport={"width": 1280, "height": 720},
            locale="ar-SA",
        )

        # Apply stealth
        try:
            from playwright_stealth import stealth_sync
            page = context.new_page()
            stealth_sync(page)
        except ImportError:
            logger.warning("âš ï¸ playwright-stealth ØºÙŠØ± Ù…Ø«Ø¨Ù‘Øª â€” Ù…ØªØ§Ø¨Ø¹Ø© Ø¨Ø¯ÙˆÙ† stealth")
            page = context.new_page()

        while True:
            elapsed = time.time() - start_time
            if not once and elapsed >= LOOP_DURATION_SECONDS:
                logger.info(f"â° Ø§Ù†ØªÙ‡Ù‰ Ø§Ù„ÙˆÙ‚Øª ({int(elapsed)}s) â€” Ø¥ÙŠÙ‚Ø§Ù")
                break

            iteration += 1
            logger.info(f"\n{'â”€' * 40}")
            logger.info(f"ğŸ”„ Ø§Ù„Ø¯ÙˆØ±Ø© #{iteration} | {int(elapsed)}s Ù…Ù† {LOOP_DURATION_SECONDS}s")
            logger.info(f"{'â”€' * 40}")

            try:
                # 1. Scrape
                matches = scrape_matches(page)
                logger.info(f"ğŸ“Š ØªÙ… Ø¬Ù„Ø¨ {len(matches)} Ù…Ø¨Ø§Ø±Ø§Ø©")

                # 2. Prioritize live matches
                live = [m for m in matches if m["status"] == "live"]
                other = [m for m in matches if m["status"] != "live"]
                sorted_matches = live + other

                if live:
                    logger.info(f"ğŸ”´ {len(live)} Ù…Ø¨Ø§Ø±Ø§Ø© Ù…Ø¨Ø§Ø´Ø±Ø©!")

                # 3. Log matches
                for m in sorted_matches:
                    status_emoji = {"live": "ğŸ”´", "finished": "ğŸ", "upcoming": "â³"}.get(
                        m["status"], "â“"
                    )
                    logger.info(
                        f"  {status_emoji} {m['home_team_name']} "
                        f"{m['home_score']}-{m['away_score']} "
                        f"{m['away_team_name']} "
                        f"({m.get('league_name', '?')})"
                    )

                # 4. Upsert to DB
                if not dry_run and sorted_matches:
                    try:
                        session = get_session()
                        stats = upsert_matches(session, sorted_matches)
                        session.close()
                        logger.info(
                            f"ğŸ’¾ DB: {stats['updated']} ØªØ­Ø¯ÙŠØ« | "
                            f"{stats['skipped']} ØªØ®Ø·ÙŠ"
                        )
                    except Exception as e:
                        logger.error(f"âŒ Ø®Ø·Ø£ ÙÙŠ Ø§Ù„Ù€ DB: {e}")

            except Exception as e:
                logger.error(f"âŒ Ø®Ø·Ø£ ÙÙŠ Ø§Ù„Ø¯ÙˆØ±Ø© #{iteration}: {e}")

            if once:
                logger.info("âœ… Ø§Ù†ØªÙ‡Ù‰ (ÙˆØ¶Ø¹ Ø§Ù„Ù…Ø±Ø© Ø§Ù„ÙˆØ§Ø­Ø¯Ø©)")
                break

            # Sleep
            remaining = LOOP_DURATION_SECONDS - (time.time() - start_time)
            sleep_time = min(SLEEP_INTERVAL_SECONDS, remaining)
            if sleep_time > 0:
                logger.info(f"ğŸ’¤ Ø§Ù†ØªØ¸Ø§Ø± {int(sleep_time)}s...")
                time.sleep(sleep_time)

        browser.close()

    logger.info("=" * 50)
    logger.info(f"ğŸ Ø§Ù†ØªÙ‡Ù‰ Ø§Ù„Ø³ÙƒØ±Ø§Ø¨Ø± â€” {iteration} Ø¯ÙˆØ±Ø© ÙÙŠ {int(time.time() - start_time)}s")
    logger.info("=" * 50)


# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# Entry Point
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

if __name__ == "__main__":
    parser = argparse.ArgumentParser(description="âš½ Kora Live Scores Scraper")
    parser.add_argument(
        "--once",
        action="store_true",
        help="ØªØ´ØºÙŠÙ„ Ù…Ø±Ø© ÙˆØ§Ø­Ø¯Ø© ÙÙ‚Ø· (Ù„Ù„Ø§Ø®ØªØ¨Ø§Ø±)",
    )
    parser.add_argument(
        "--dry-run",
        action="store_true",
        help="Ø¬Ù„Ø¨ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø¨Ø¯ÙˆÙ† ØªØ­Ø¯ÙŠØ« Ù‚Ø§Ø¹Ø¯Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª",
    )
    args = parser.parse_args()

    run_scraper(once=args.once, dry_run=args.dry_run)
